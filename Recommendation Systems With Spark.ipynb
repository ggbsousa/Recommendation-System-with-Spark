{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System with Spark\n",
    "By Gabriel Garcez Barros Sousa\n",
    "\n",
    "Dataset acquired from [GroupLens](http://grouplens.org/datasets/movielens/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation systems are a hot topic nowadays and are pretty much ubiquitous, being seen in online stores, movie databases and even job finders. In this Notebook, we will study the use of Spark's Machine Learning Library mllib to build a Collaborative Filtering based recommendation system using the [Alternating Least Squares](http://cs229.stanford.edu/proj2014/Christopher%20Aberger,%20Recommender.pdf) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Acquiring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acquire and extract the data, simply run the following Bash scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget -O moviedataset.zip http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!unzip -o moviedataset.zip -d /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, let's get all of the imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dataframe manipulation library and machine learning model\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "#Math functions. We'll only need the sqrt function so let's import only that\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's read the file and extract the information that we need.\n",
    "\n",
    "For the movies file, let's only extract the movieId and its title since we won't be using genres for this recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load movies.dat in with Spark\n",
    "movies_raw = sc.textFile('/resources/data/ml-1m/movies.dat')\n",
    "#Take the first line from the raw to get the header names\n",
    "movies_header = movies_raw.take(1)[0]\n",
    "#Now clean up the data by first filtering out the header.\n",
    "#Then split every line by '::'.\n",
    "#And finally, extract only the data we need, which are the movieId and title, \n",
    "#so only get the first two elements of what we get after splitting\n",
    "#And also convert the movieId to an int\n",
    "movies_data = movies_raw.filter(lambda line: line!=movies_header)\\\n",
    "    .map(lambda line: line.split(\"::\")).map(lambda tokens: (int(tokens[0]), tokens[1])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the data structure looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, u'Jumanji (1995)'),\n",
       " (3, u'Grumpier Old Men (1995)'),\n",
       " (4, u'Waiting to Exhale (1995)'),\n",
       " (5, u'Father of the Bride Part II (1995)'),\n",
       " (6, u'Heat (1995)')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's load in the ratings files and extract the userId, movieId and the rating the user gave to that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load in the ratings file\n",
    "ratings_raw = sc.textFile('/resources/data/ml-1m/ratings.dat')\n",
    "#Take the first line from the raw to get the header names\n",
    "ratings_header = ratings_raw.take(1)[0]\n",
    "#Now clean up the data by first filtering out the header.\n",
    "#Then split every line by '::'.\n",
    "#And finally, extract only the data we need, which are the userId, movieId and rating \n",
    "#so only get the first three elements of what we get after splitting\n",
    "ratings_data = ratings_raw.filter(lambda line: line!=ratings_header)\\\n",
    "    .map(lambda line: line.split(\"::\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the final ratings data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 661, 3.0), (1, 914, 3.0), (1, 3408, 4.0), (1, 2355, 5.0), (1, 1197, 3.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tuning the Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start training our recommendation model. But before we actually train the final model, we have to find out what parameters are optimal for it by training the model with varying parameters and selecting the best configuration.\n",
    "\n",
    "Let's do this by first getting a random sample with a tenth of the whole dataset and then splitting that sample into training and validation data.\n",
    "\n",
    "The training data will train the model and the validation data will be used to test the model and measure the error. The configuration that presents the smallest error is the one that possesses the best configuration. The function we will be using to measure the error known as the Root Mean Square Error (RMSE) function which is seen below:\n",
    "\n",
    "RMSE = $\\sqrt{\\frac{1}{n}\\sum_{t=1}^{n}e_t^2}$\n",
    "Where e is the error for that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's split our data into training and validation data for machine learning\n",
    "randomSample = sc.parallelize(ratings_data.takeSample(False,ratings_data.count()/10))\n",
    "training, validation = randomSample.randomSplit([0.8, 0.2])\n",
    "#We only need the first two elements for the validation prediction, so let's extract it\n",
    "validationPredict = validation.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's now set the hyper parameters for training the model.\n",
    "\n",
    "Spark's Alternating Least Squares function takes in these parameters:\n",
    "\n",
    "* Iterations: Number of iterations to run the model.\n",
    "* Rank: Number of latent factors in the model.\n",
    "* Lambda: The regularization parameter used in ALS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parameter definition for Alternating Least Squares\n",
    "#You can play around with these values to find the best configuration\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "\n",
    "#Variables for error tracking\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model using different parameters and find out which configuration is best one based on error.\n",
    "\n",
    "* model stores the trained model.\n",
    "* predictions stores the predictions given by the model for a given userID and movieID.\n",
    "* ratings_and_preds stores the the predictions and the actual rating given by the user.\n",
    "* error stores the error value given after calculating the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was trained with rank 4 and possessed error: 1.07857459098\n",
      "Every error value produced: [1.0785745909816142, 1.0814910411108178, 1.0812052119806697]\n"
     ]
    }
   ],
   "source": [
    "errorList = []\n",
    "#Now let's figure out how many latent variables we should use by training it several times and changing the rank value each time\n",
    "for rank in ranks:\n",
    "    #Train the machine and get the model\n",
    "    model = ALS.train(training, rank, iterations=iterations, lambda_=regularization_parameter)\n",
    "    #Now let's use the validation data to have the machine predict ratings\n",
    "    #r[0] is the userId, r[1] is the movieId and r[2] is the predicted value\n",
    "    predictions = model.predictAll(validationPredict).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    #Now let's get the actual results of the validation data so we can compute the error\n",
    "    ratings_and_preds = validation.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    #We're computing the error with the Root Mean Squares Error fnction. (RMSE)\n",
    "    error = sqrt(ratings_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    #Now, we check to see if the current error was the lowest one\n",
    "    #if so, store it and the current configuration that we used\n",
    "    errorList.append(error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print 'The best model was trained with rank ' + str(best_rank) + ' and possessed error: ' + str(min_error)\n",
    "print 'Every error value produced: ' + str(errorList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, let's use our best configuration to train the ALS model on our full dataset while using part of it for testing.\n",
    "\n",
    "So, let's repeat the process used for creating the training and validation data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test = ratings_data.randomSplit([0.8, 0.2])\n",
    "#We only need the first two elements for testing so let's extract it\n",
    "testPredict = test.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With the training and test data set, let's train the model and see its error value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.877947641725.\n"
     ]
    }
   ],
   "source": [
    "#Retrain it using the best found configuration\n",
    "full_model = ALS.train(training, best_rank, iterations=iterations, lambda_=regularization_parameter)\n",
    "#Predict the ratings for the test data\n",
    "predictions = full_model.predictAll(testPredict).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "#Get the actual ratings from the test data\n",
    "ratings_and_preds = test.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "#Now calculate the RMSE\n",
    "error = sqrt(ratings_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "print 'For testing data the RMSE is ' + str(error) + '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the parameters set, let's start recommending movies to an input user.\n",
    "\n",
    "Let's start by defining an input user in the same format as everything in the ratings data structure. But with an ID that isn't in the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "#Insert a new user to recommend movies to here\n",
    "#The format of each line is (userID, movieID, rating)\n",
    "new_user = [\n",
    "     (0, 1, 3.5),  #Toy Story\n",
    "     (0, 2, 2), # Jumanji\n",
    "     (0, 296, 5), # Pulp Fiction\n",
    "     (0, 1274, 4.5), # Akira\n",
    "     (0, 1968, 5) # The Breakfast Club\n",
    "    ]\n",
    "new_user_ratings = sc.parallelize(new_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a way of counting the amount of ratings per movie so we can recommend movies with at least a certain amount of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Form groups by movies\n",
    "movie_ID_with_ratings = (ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "#Find the average score and amount of ratings for every movie then get the amount of ratings per movie\n",
    "movie_rating_counts = movie_ID_with_ratings.map(lambda x : (x[0], len(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to add the input user to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a new dataframe holding every rating including the new user\n",
    "data_with_new_user = ratings_data.union(new_user_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the model with the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#And train it\n",
    "new_ratings_model = ALS.train(data_with_new_user, best_rank, iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we recommend something to our input user, let's get the movies our user hasn't watched and run it through the prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the input user's movie id\n",
    "new_user_ratings_ids = map(lambda x: x[1], new_user)\n",
    "#Now filter out the movies that the user has watched\n",
    "new_user_unrated_movies = (movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "#Now predict input user's movie ratings\n",
    "new_user_recommendations = new_ratings_model.predictAll(new_user_unrated_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our recommendations look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=1084, rating=3.874892121279405),\n",
       " Rating(user=0, product=3456, rating=3.4479192521546027),\n",
       " Rating(user=0, product=3764, rating=2.232790850369536),\n",
       " Rating(user=0, product=3272, rating=3.002177863628324),\n",
       " Rating(user=0, product=1724, rating=3.1338470117069774),\n",
       " Rating(user=0, product=428, rating=4.123986330616036),\n",
       " Rating(user=0, product=1900, rating=3.5348965411706272),\n",
       " Rating(user=0, product=1328, rating=1.3924031856122951),\n",
       " Rating(user=0, product=464, rating=2.5028695946299937),\n",
       " Rating(user=0, product=1040, rating=2.5440409840778058)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_recommendations.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reformat the data structure, include the amount of ratings and clean it up a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now reformat the new_user_recommendations dataframe to the form of (movieID, Predicted Rating)\n",
    "new_user_recommendations = new_user_recommendations.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations = new_user_recommendations.join(movies_data).join(movie_rating_counts)\n",
    "#Clean it up a bit\n",
    "new_user_recommendations = new_user_recommendations.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all that's left is to recommend something, so let's filter out movies with less than 20 reviews and order them from highest recommended to lowest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 recommended movies with over 20 reviews:\n",
      "(u'For All Mankind (1989)', 4.878450594112355, 27)\n",
      "(u'Dear Diary (Caro Diario) (1994)', 4.862527061004506, 28)\n",
      "(u'Sanjuro (1962)', 4.860741545687377, 69)\n",
      "(u'American Beauty (1999)', 4.8193084503413655, 3428)\n",
      "(u'Usual Suspects, The (1995)', 4.792900578839774, 1783)\n",
      "(u'GoodFellas (1990)', 4.739372913523984, 1657)\n",
      "(u'Godfather, The (1972)', 4.735397920456402, 2223)\n",
      "(u'Last Days, The (1998)', 4.698700108200825, 27)\n",
      "(u'Reservoir Dogs (1992)', 4.686128645014773, 1259)\n",
      "(u'Monty Python and the Holy Grail (1974)', 4.654967948786981, 1599)\n",
      "(u'Idiots, The (Idioterne) (1998)', 4.648629307901874, 37)\n",
      "(u'Shawshank Redemption, The (1994)', 4.639626104173599, 2227)\n",
      "(u'Apocalypse Now (1979)', 4.634400637504494, 1176)\n",
      "(u'Fight Club (1999)', 4.619109706608868, 1451)\n",
      "(u'Matrix, The (1999)', 4.591628605400125, 2590)\n",
      "(u'Godfather: Part II, The (1974)', 4.590559252383079, 1692)\n",
      "(u'Star Wars: Episode IV - A New Hope (1977)', 4.559225143047281, 2991)\n",
      "(u'Clockwork Orange, A (1971)', 4.558950226992932, 1229)\n",
      "(u'Full Metal Jacket (1987)', 4.533923475143698, 1254)\n",
      "(u'Clerks (1994)', 4.524673410454555, 1412)\n",
      "(u\"One Flew Over the Cuckoo's Nest (1975)\", 4.520212572717544, 1724)\n",
      "(u'Animal House (1978)', 4.504484370787313, 1207)\n",
      "(u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 4.502412031518785, 1367)\n",
      "(u'Requiem for a Dream (2000)', 4.481516910898554, 304)\n",
      "(u'Silence of the Lambs, The (1991)', 4.481385138590992, 2578)\n"
     ]
    }
   ],
   "source": [
    "#Now get the top 25 movies with over 20 ratings and display them in order from highest recommended to lowest\n",
    "top_movies = new_user_recommendations.filter(lambda r: r[2]>=20).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "print ('Top 20 recommended movies with over 20 reviews:\\n%s' % '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Gabriel Garcez Barros Sousa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Recommender: An Analysis of Collaborative Filtering Techniques](http://cs229.stanford.edu/proj2014/Christopher%20Aberger,%20Recommender.pdf)\n",
    "* [Root Mean Square Error](https://www.kaggle.com/wiki/RootMeanSquaredError)\n",
    "* [Alternating Least Squares for Collaborative Filtering](http://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/)\n",
    "* [ALS function](https://spark.apache.org/docs/0.8.1/api/mllib/org/apache/spark/mllib/recommendation/ALS)\n",
    "* [MLib](http://spark.apache.org/docs/latest/mllib-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
